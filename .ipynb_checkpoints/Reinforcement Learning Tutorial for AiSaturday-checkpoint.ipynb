{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning how to use simple Q learning \n",
    "## @ AISaturdays Singapore\n",
    "####  By Nasrudin Salim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## What is Q learning?\n",
    "\n",
    "Q-learning is a model-free reinforcement learning technique. Specifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function, often denoted by Q ( s , a ) , which ultimately gives the expected utility of taking a given action a in a given state s, and following an optimal policy thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    A = Action\n",
    "    S = State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    Alpha - Learning Rate\n",
    "    Episilon  - Greedy Policy\n",
    "    Gamma - Discount Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving 1 Dimensional game problem\n",
    "\n",
    "### Reinforcement learning  using table-lookup Q learning method\n",
    "\n",
    "#### Problem\n",
    "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
    "Create an algorithm to allow the agent to solve the problem and go towards the treasure\n",
    "\n",
    "O----------T\n",
    "\n",
    "##### Q-learning\n",
    "Model-free learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)  # reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create the problem/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "\n",
    "# The available actions to perform\n",
    "ACTIONS = ['left', 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Build q table\n",
    "The simplest Q learning uses a table to store data, a q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    ''' Create a matrix, with amount of columns = amount of actions available\n",
    "    N_states refers to the states available in the environment.\n",
    "    We will have a row for each possible state'''\n",
    "    \n",
    "    #Create a pandas dataframe of size (nstates * amount of actions) \n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    print(table)    # show table\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "q_table = build_q_table(N_STATES,ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Function to create and update the environment for human's observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def update_env(State, episode, step_counter):\n",
    "    ''' This is how the environment is updated and drawn/printed out for humans to see\n",
    "    Basically draws out 'o------------------T' and where o currently is\n",
    "    '''\n",
    "    \n",
    "    #Create the base environment without the location of the agent\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    \n",
    "    #If the current state has reached the treasure, do this\n",
    "    if State == 'treasure':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(0.5) #add a delay so humans can observe\n",
    "        print('\\r                                ', end='')\n",
    "    #if the state is still numeric, means it's somewhere other than at the treasure\n",
    "    else:\n",
    "        #We draw the \"o\" at where the agent is right now and then print it out so humans can see where it is\n",
    "        env_list[State] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(0.1) #add a delay so humans can observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the agent\n",
    "\n",
    "Allow it to see the environment, the action states and then make an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    '''This is how to choose an action, looking up the q table and observe the current state,\n",
    "    find out what action is best to take at this state'''\n",
    "    \n",
    "    #Look up where it is now\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    \n",
    "    # act non-greedy or state-action have no value\n",
    "    #When the state action amount isn't useful, negligible ~ on what definite action to take at this state\n",
    "    #Perform a random action\n",
    "    if (np.random.uniform() > EPSILON) or (state_actions.all() == 0):  \n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    #The action probability is high enough at this state(Knows what definite action to do), act greedy.\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()   \n",
    "    return action_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Agent decides on an action, then updates the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_env_feedback(State, Action):\n",
    "    ''' This is how agent will interact with the environment, the agent makes an action\n",
    "    we then update the new state it is now in, or whether it has reached the treasure'''\n",
    "    if Action == 'right':    # move right\n",
    "        if State == N_STATES - 2:   # If moving right would hit the end of the 1 dimensional world,\n",
    "                                # Which is where the treasure is, means it has reached the treasure\n",
    "            State = 'treasure'\n",
    "            ReachedTreasure = True\n",
    "        #Move right, but not yet reached treasure\n",
    "        else:\n",
    "            State += 1 #This is where it is now\n",
    "            ReachedTreasure = False\n",
    "            \n",
    "    #Moving left\n",
    "    elif Action== 'left':  \n",
    "        ReachedTreasure = False #Moving left does not reach any treasure\n",
    "        if State != 0: # Did not reach a wall\n",
    "            \n",
    "            State -= 1 #this is where it is now\n",
    "            \n",
    "    return State, ReachedTreasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterating through the RL agent and the environment with a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "MAX_EPISODES = 14   # maximum episodes\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "\n",
    "# Edit these parameters to create more efficient Q learning RL agents.\n",
    "    #E.g RandomSearch optimization etc\n",
    "EPSILON = 0.9   # greedy policy\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "                                       left     right\n",
      "0  0.000040  0.033417\n",
      "1  0.000025  0.110328\n",
      "2  0.000051  0.294569\n",
      "3  0.026637  0.569068\n",
      "4  0.044317  0.878423\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "q_table = build_q_table(N_STATES, ACTIONS)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    step_counter = 0\n",
    "    State = 0\n",
    "    is_terminated = False\n",
    "    #Draw out the environment on the console/ output for humans to see\n",
    "    update_env(State, episode, step_counter)\n",
    "    while not is_terminated:\n",
    "\n",
    "        Action = choose_action(State, q_table)\n",
    "        S_, ReachedTreasure = get_env_feedback(State, Action)  # take action & get next state and reward\n",
    "        q_predict = q_table.loc[State, Action]\n",
    "        if S_ != 'treasure':\n",
    "            q_target = ReachedTreasure + GAMMA * q_table.iloc[S_, :].max()   # next state is not treasure\n",
    "        else:\n",
    "            q_target = ReachedTreasure     # next state is treasure\n",
    "            is_terminated = True    # terminate this episode\n",
    "\n",
    "        q_table.loc[State, Action] += ALPHA * (q_target - q_predict)  # update\n",
    "        State = S_  # move to next state\n",
    "\n",
    "        update_env(State, episode, step_counter+1)\n",
    "        step_counter += 1\n",
    "\n",
    "\n",
    "#save the trained q_table into a pickle, for next run\n",
    "q_table.to_pickle(\"trained_q_table.p\")\n",
    "\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "     left  right\n",
    "    0   0.0    0.0\n",
    "    1   0.0    0.0\n",
    "    2   0.0    0.0\n",
    "    3   0.0    0.0\n",
    "    4   0.0    0.0\n",
    "    5   0.0    0.0\n",
    "    \n",
    "    left     right\n",
    "    0  0.000001  0.005095\n",
    "    1  0.000001  0.028124\n",
    "    2  0.000035  0.119756\n",
    "    3  0.000073  0.363412\n",
    "    4  0.019316  0.745813\n",
    "    5  0.000000  0.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### If you want to load a saved trained q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def StartNew(New=True):\n",
    "    if New:\n",
    "        #Start afresh, so we create an empty q_table\n",
    "        q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    else:\n",
    "        #Load a pretrained q_table\n",
    "        with open('trained_q_table.p', 'rb') as f:\n",
    "            q_table = pk.load(f)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6: total_steps = 5      "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-66dc9f25a17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_\u001b[0m  \u001b[0;31m# move to next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mupdate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_counter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstep_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ab7e289971be>\u001b[0m in \u001b[0;36mupdate_env\u001b[0;34m(State, episode, step_counter)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minteraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Episode %s: total_steps = %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add a delay so humans can observe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r                                '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#if the state is still numeric, means it's somewhere other than at the treasure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_table = StartNew(False)\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    step_counter = 0\n",
    "    State = 0\n",
    "    is_terminated = False\n",
    "    #Draw out the environment on the console/ output for humans to see\n",
    "    update_env(State, episode, step_counter)\n",
    "    while not is_terminated:\n",
    "\n",
    "        Action = choose_action(State, q_table)\n",
    "        S_, ReachedTreasure = get_env_feedback(State, Action)  # take action & get next state and reward\n",
    "        q_predict = q_table.loc[State, Action]\n",
    "        if S_ != 'treasure':\n",
    "            q_target = ReachedTreasure + GAMMA * q_table.iloc[S_, :].max()   # next state is not treasure\n",
    "        else:\n",
    "            q_target = ReachedTreasure     # next state is treasure\n",
    "            is_terminated = True    # terminate this episode\n",
    "\n",
    "        q_table.loc[State, Action] += ALPHA * (q_target - q_predict)  # update\n",
    "        State = S_  # move to next state\n",
    "\n",
    "        update_env(State, episode, step_counter+1)\n",
    "        step_counter += 1\n",
    "\n",
    "\n",
    "#save the trained q_table into a pickle, for next run\n",
    "q_table.to_pickle(\"trained_q_table.p\")\n",
    "\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
